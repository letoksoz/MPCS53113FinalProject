{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys, csv, datetime, time, json\n",
    "from os.path import expanduser, exists\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.layers import * \n",
    "from keras.activations import softmax\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.stem import SnowballStemmer\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "urllib.request.urlretrieve(\"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\", \"quora_duplicate_questions.tsv\")\n",
    "import pandas as pd\n",
    "data = pd.read_csv('quora_duplicate_questions.tsv', sep='\\t')\n",
    "list(data.columns)\n",
    "#Remove samples with nan\n",
    "import numpy as np\n",
    "dataq1 = data['question1']\n",
    "dataq2 = data['question2']\n",
    "\n",
    "q1_nans = np.where(dataq1.isnull())[0]\n",
    "q2_nans = np.where(dataq2.isnull())[0]\n",
    "nan_indeces = np.concatenate([q1_nans,q2_nans])\n",
    "print(\"Print NAN indices:\",nan_indeces)\n",
    "\n",
    "did = data['id']\n",
    "data = data.drop(nan_indeces)\n",
    "data = data[['question1', 'question2','is_duplicate']]\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#global variables\n",
    "MAX_NB_WORDS = 2000000\n",
    "MAX_SEQUENCE_LENGTH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question1 = []\n",
    "question2 = []\n",
    "is_duplicate = []\n",
    "with open('quora_duplicate_questions.tsv') as csvfile:\n",
    "    reader = csv.DictReader(csvfile, delimiter='\\t')\n",
    "    for row in reader:\n",
    "        question1.append(row['question1'])\n",
    "        question2.append(row['question2'])\n",
    "        is_duplicate.append(row['is_duplicate'])\n",
    "print('Question pairs: %d' % len(question1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Build tokenized word index\n",
    "questions = question1 + question2\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "#fit_on_texts counts the occurrence of each word in the vocab\n",
    "tokenizer.fit_on_texts(questions)\n",
    "question1_word_sequences = tokenizer.texts_to_sequences(question1)\n",
    "question2_word_sequences = tokenizer.texts_to_sequences(question2)\n",
    "word_index = tokenizer.word_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = {}\n",
    "#Prepare word embeddings dictionary\n",
    "with open(\"glove.6B.300d.txt\") as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "print('Word embeddings: %d' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepare word embedding matrix\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))\n",
    "word_embedding_matrix = np.zeros((nb_words + 1, 300)) #300-dimensions\n",
    "for word, i in word_index.items():\n",
    "    if i > MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        word_embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        word_embedding_matrix[i] = np.random.rand(1, 300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data tensors\n",
    "q1_data = pad_sequences(question1_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "q2_data = pad_sequences(question2_word_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(is_duplicate, dtype=int)\n",
    "print('Shape of question1 data tensor:', q1_data.shape)\n",
    "print('Shape of question2 data tensor:', q2_data.shape)\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Persist training and configuration data to files\n",
    "np.save(open(\"Q1_training_data.npy\", 'wb'), q1_data)\n",
    "np.save(open(\"Q2_training_data.npy\", 'wb'), q2_data)\n",
    "np.save(open(\"label_training_data.npy\", 'wb'), labels)\n",
    "np.save(open(\"word_embedding_matrix.npy\", 'wb'), word_embedding_matrix)\n",
    "with open(\"nb_words.json\", 'w') as f:\n",
    "    json.dump({'nb_words': nb_words}, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.stdout.flush()\n",
    "# Partition the dataset into train and test sets, train test split = 0.2\n",
    "X = np.stack((q1_data, q2_data), axis=1)\n",
    "y = labels\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12345)\n",
    "Q1_train = X_train[:,0]\n",
    "Q2_train = X_train[:,1]\n",
    "Q1_test = X_test[:,0]\n",
    "Q2_test = X_test[:,1]\n",
    "\n",
    "print(\"Q1_train.shape: \", Q1_train.shape)\n",
    "print(\"Q2_train.shape :\", Q2_train.shape)\n",
    "print(\"Q1_test.shape: \", Q1_test.shape)\n",
    "print(\"Q2_test.shape :\", Q2_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#helper function for definint attention model\n",
    "def unchanged_shape(input_shape):\n",
    "    return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model\n",
    "question1 = Input(shape=(100,))\n",
    "question2 = Input(shape=(100,))\n",
    "q1 = Embedding(nb_words + 1, \n",
    "                 300, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=100, \n",
    "                 trainable=False)(question1)\n",
    "print(\"q1 shape :\", q1.shape)\n",
    "\n",
    "#200 sentence embedding dimension\n",
    "q1 = Bidirectional(LSTM(200, return_sequences=True))(q1)\n",
    "\n",
    "print(\"q1 shape :\", q1.shape)\n",
    "q2 = Embedding(nb_words + 1, \n",
    "                 300, \n",
    "                 weights=[word_embedding_matrix], \n",
    "                 input_length=100, \n",
    "                 trainable=False)(question2)\n",
    "print(\"q2 shape :\", q2.shape)\n",
    "\n",
    "q2 = Bidirectional(LSTM(200, return_sequences=True))(q2)\n",
    "\n",
    "attention = Dot(axes=-1)([q1, q2])\n",
    "w_att_1 = Lambda(lambda x: softmax(x, axis=1), output_shape=unchanged_shape)(attention)\n",
    "w_att_2 = Permute((2,1))(Lambda(lambda x: softmax(x, axis=2), output_shape=unchanged_shape)(attention))\n",
    "q1_aligned = Dot(axes=1)([w_att_1, q1])\n",
    "q2_aligned = Dot(axes=1)([w_att_2, q2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Get multiplication and subtraction, concatenate results\n",
    "def submult(i1, i2):\n",
    "    mult = Multiply()([i1, i2])\n",
    "    sub = substract(i1, i2)\n",
    "    out= Concatenate()([sub, mult])\n",
    "    return out\n",
    "\n",
    "#Substract element-wise\n",
    "def substract(i1, i2):\n",
    "    negative_i2 = Lambda(lambda x: -x, output_shape=unchanged_shape)(i2)\n",
    "    out = Add()([i1, negative_i2])\n",
    "    return out\n",
    "\n",
    "#Apply layers to input, concatenate result\n",
    "def apply_multiple(original_input, layers):\n",
    "    agg = []\n",
    "    for layer in layers:\n",
    "        agg.append(layer(original_input))\n",
    "    out = Concatenate()(agg)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "q1_combined = concatenate([q1, q2_aligned, submult(q1, q2_aligned)])\n",
    "q2_combined = concatenate([q2, q1_aligned, submult(q2, q1_aligned)]) \n",
    "compose = Bidirectional(LSTM(200, return_sequences=True))\n",
    "q1_compare = compose(q1_combined)\n",
    "q2_compare = compose(q2_combined)\n",
    "\n",
    "q1_rep = apply_multiple(q1_compare, [GlobalAvgPool1D(), GlobalMaxPool1D()])\n",
    "q2_rep = apply_multiple(q2_compare, [GlobalAvgPool1D(), GlobalMaxPool1D()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged = concatenate([q1_rep, q2_rep])\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(1000, activation='elu')(merged)\n",
    "\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(500, activation='elu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(200, activation='elu')(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "merged = Dense(100, activation='elu')(merged)\n",
    "merged = Dropout(0.1)(merged)\n",
    "merged = BatchNormalization()(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_duplicate = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def function(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[question1, question2], outputs=is_duplicate)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', function])\n",
    "print(\"Starting training at\", datetime.datetime.now())\n",
    "sys.stdout.flush()\n",
    "t0 = time.time()\n",
    "callbacks = [ModelCheckpoint(\"question_pairs_weights_attention\", monitor='val_acc', save_best_only=True)]\n",
    "history = model.fit([Q1_train, Q2_train],\n",
    "                    y_train,\n",
    "                    epochs=20,\n",
    "                    validation_split=0.2,\n",
    "                    verbose=2,\n",
    "                    batch_size=500,\n",
    "                    callbacks=callbacks)\n",
    "t1 = time.time()\n",
    "print(\"Training ended at\", datetime.datetime.now())\n",
    "print(\"Minutes elapsed: %f\" % ((t1 - t0) / 60.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print best validation accuracy and associated epoch\n",
    "max_acc, idx = max((val, idx) for (idx, val) in enumerate(history.history['val_acc']))\n",
    "print('Maximum validation accuracy = {0:.4f} (epoch {1:d})'.format(max_acc, idx+1))\n",
    "\n",
    "# Evaluate the model with best validation accuracy on the test partition\n",
    "model.load_weights(\"question_pairs_weights_attention\")\n",
    "loss, accuracy, function = model.evaluate([Q1_test, Q2_test], y_test, verbose=0)\n",
    "print(model.evaluate([Q1_test, Q2_test], y_test, verbose=0))\n",
    "sys.stdout.flush()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
