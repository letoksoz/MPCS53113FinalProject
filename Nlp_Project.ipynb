{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "7NWf3yZPenBS",
    "outputId": "a3b3333a-850e-4a6a-e64b-38e47c1b4265"
   },
   "outputs": [],
   "source": [
    "import urllib\n",
    "urllib.request.urlretrieve(\"http://qim.fs.quoracdn.net/quora_duplicate_questions.tsv\", \"quora_duplicate_questions.tsv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "GD7_baXienBZ",
    "outputId": "7fce3c3c-f525-4564-946b-2ceca1bb76be"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('quora_duplicate_questions.tsv', sep='\\t')\n",
    "list(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "aI0PyskhenBe",
    "outputId": "39ad8a5c-4942-4fb5-e4bf-05838b702456"
   },
   "outputs": [],
   "source": [
    "#Remove samples with nan\n",
    "import numpy as np\n",
    "dataq1 = data['question1']\n",
    "dataq2 = data['question2']\n",
    "\n",
    "q1_nans = np.where(dataq1.isnull())[0]\n",
    "q2_nans = np.where(dataq2.isnull())[0]\n",
    "nan_indeces = np.concatenate([q1_nans,q2_nans])\n",
    "print(\"Print NAN indices:\",nan_indeces)\n",
    "\n",
    "did = data['id']\n",
    "data = data.drop(nan_indeces)\n",
    "data = data[['question1', 'question2','is_duplicate']]\n",
    "data.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "3_GhBCFeh5MS",
    "outputId": "b6d1f570-cbaa-4c9a-e178-b7596a734c7c"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "inputs, test_set = train_test_split(data, test_size=0.2)\n",
    "train_set, val_set = train_test_split(inputs, test_size=0.2)\n",
    "print(\"Train shape:\", train_set.shape)\n",
    "print(\"Test shape:\",test_set.shape)\n",
    "print(\"Val shape:\",val_set.shape)\n",
    "#Modifiying the sizes so shape is divisible by the batchsize\n",
    "train_set = train_set.append(train_set[:9])\n",
    "print(\"Modified Train shape:\", train_set.shape)\n",
    "test_set = test_set.append(test_set[:6])\n",
    "print(\"Modified Test shape:\",test_set.shape)\n",
    "val_set = val_set.append(val_set[:18])\n",
    "print(\"Modified Val shape:\",val_set.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-qWegGRB5CeZ"
   },
   "outputs": [],
   "source": [
    "train_set.to_csv(\"train.csv\",index = False)\n",
    "val_set.to_csv(\"val.csv\",index = False)\n",
    "test_set.to_csv(\"test.csv\",index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tbZ-kJHeenBj",
    "outputId": "e0e0907f-29e5-457d-d16f-3ab498cac569"
   },
   "outputs": [],
   "source": [
    "trn_q1_set = train_set['question1'].values\n",
    "trn_q2_set = train_set['question2'].values\n",
    "trn_qcombined_set = np.concatenate((trn_q1_set, trn_q2_set), axis=0)\n",
    "print(\"Combined question set shape:\",trn_qcombined_set.shape)\n",
    "print(\"Sample question from the set:\",trn_qcombined_set[19])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "scCOX-uLenBo"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "spacy_en = spacy.load('en')\n",
    "\n",
    "def tokenizer(text): \n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]\n",
    "\n",
    "trn_qcombined_len = len(trn_qcombined_set)\n",
    "trn_word_list = set()\n",
    "for i in range(trn_qcombined_len):\n",
    "    for words in tokenizer(trn_qcombined_set[i]):\n",
    "        trn_word_list.add(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "1WTHPBOYenBt",
    "outputId": "233587bc-0126-49ba-c727-69b47f1de062"
   },
   "outputs": [],
   "source": [
    "print(\"Unique Word Count:\", len(trn_word_list))\n",
    "MAX_VOCAB_SIZE = int(len(trn_word_list))\n",
    "print(\"Max Vocab Size:\", MAX_VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "fgW3f0lwenBz"
   },
   "outputs": [],
   "source": [
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "TEXT = data.Field(sequential=True, \n",
    "                       tokenize='spacy',  \n",
    "                       use_vocab=True,\n",
    "                       lower=True)\n",
    "                 \n",
    "LABELS = data.Field(sequential=False, \n",
    "                         use_vocab=False, \n",
    "                         pad_token=None, \n",
    "                         unk_token=None)\n",
    "\n",
    "data_fields = [\n",
    "    ('question1', TEXT),\n",
    "    ('question2', TEXT), \n",
    "    ('is_duplicate', LABELS) \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "mP9X1RQkyvno"
   },
   "outputs": [],
   "source": [
    "train, val, test = data.TabularDataset.splits(path='.', \n",
    "                                            format='csv', \n",
    "                                            train='train.csv', \n",
    "                                            validation='val.csv',\n",
    "                                            test='test.csv',\n",
    "                                            fields=data_fields, \n",
    "                                            skip_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DeaMaYKOenB4",
    "outputId": "caea4c9d-597a-4e12-a7ed-0146c48f9fa2"
   },
   "outputs": [],
   "source": [
    "print(\"Length of the training set:\",len(train))\n",
    "ex = train[0]\n",
    "print(\"Q1 field of the first sample:\\n\",ex.question1)\n",
    "print(\"Q2 field of the first sample:\\n\",ex.question2)\n",
    "print(\"Label field of the first sample:\",ex.is_duplicate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "9zWJaYDbenB9",
    "outputId": "f8f1aaeb-cddf-4ed4-8c0b-0c1a3b301067"
   },
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE, vectors=\"glove.6B.300d\")\n",
    "print(\"Vocabulary size: {}\".format(len(TEXT.vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "i-sYERbeenCD",
    "outputId": "9384501d-91e8-4458-9708-3ab4b8a2fe08"
   },
   "outputs": [],
   "source": [
    "print(\"Word Vector of the:\")\n",
    "print(TEXT.vocab.vectors[TEXT.vocab.stoi['the']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "k3511KbXbmA7",
    "outputId": "20c67ffe-6f48-414e-8664-2c11c4fbdaf0"
   },
   "outputs": [],
   "source": [
    "print(TEXT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "e6ChfkGT_yzc"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "BATCH_SIZE = 32\n",
    "VOCAB_SIZE = len(TEXT.vocab)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_iter, val_iter, test_iter = data.BucketIterator.splits(datasets=(train, val, test),  \n",
    "                                                             batch_size=BATCH_SIZE, \n",
    "                                                             device= device,\n",
    "                                                             sort = False,\n",
    "                                                             repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "2MI_PWCtenCS",
    "outputId": "568a3ea2-5408-4bf5-a7db-80c7e59ef7c4"
   },
   "outputs": [],
   "source": [
    "from torchtext.vocab import Vectors\n",
    "print(\"Per batch length of train,val and test set:\")\n",
    "print(len(train_iter), len(val_iter),len(test_iter))\n",
    "print(TEXT.vocab.vectors[TEXT.vocab.stoi['the']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "YLRL8zUNenCa"
   },
   "outputs": [],
   "source": [
    "#Glove Based Embeddings \n",
    "class Model1(nn.Module):\n",
    "    def __init__(self, input_dim, n_hidden1, n_hidden2, n_hidden3, n_out):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.qemb = nn.Embedding(VOCAB_SIZE,300)\n",
    "        self.qemb.weight = torch.nn.Parameter(TEXT.vocab.vectors, requires_grad = False)\n",
    "        \n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        self.l1 =nn.Linear(self.input_dim, self.n_hidden1)\n",
    "        self.l2 = nn.Linear(self.n_hidden1, self.n_hidden2)\n",
    "        self.l3 = nn.Linear(self.n_hidden2, self.n_hidden3)\n",
    "        self.out = nn.Linear(self.n_hidden3, n_out)\n",
    "        \n",
    "        self.l1.weight.data.uniform_(-1, 1)\n",
    "        self.l2.weight.data.uniform_(-1, 1)\n",
    "        self.l3.weight.data.uniform_(-1, 1)\n",
    "        self.out.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def forward(self, q1, q2):\n",
    "        q1out = self.qemb(q1)\n",
    "        q2out = self.qemb(q2)\n",
    "        q1output = q1out.mean(0)\n",
    "        q2output = q2out.mean(0)\n",
    "        output = torch.cat([q1output,q2output],dim=1)\n",
    "        output = torch.tanh(self.l1(output))\n",
    "        output = torch.tanh(self.l2(output))\n",
    "        output = torch.tanh(self.l3(output))\n",
    "        return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "K3M5w8AgenCf"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float()  \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "oq4k9KitenCk"
   },
   "outputs": [],
   "source": [
    "#Training Module for all the models\n",
    "class Training_module():\n",
    "    def __init__(self, model,useRNN):\n",
    "        self.useRNN = useRNN\n",
    "        self.model = model\n",
    "        self.loss_fn = nn.BCEWithLogitsLoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters())\n",
    "    \n",
    "    def train_epoch(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "        \n",
    "        if (self.useRNN == True):\n",
    "            hidden = self.model.init_hidden(BATCH_SIZE, requires_grad = True)\n",
    "            \n",
    "        for batch in iterator:\n",
    "            Q1 = batch.question1\n",
    "            Q2 = batch.question2\n",
    "            y = batch.is_duplicate\n",
    "            \n",
    "            y = y.float()\n",
    "            \n",
    "            self.optimizer.zero_grad()\n",
    "            preds = self.model(Q1,Q2).squeeze(1)\n",
    "            loss = self.loss_fn(preds, y)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            epoch_loss += loss.item()      \n",
    "            acc = binary_accuracy(preds, y)\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "    \n",
    "    def train_model(self, train_iterator, dev_iterator):\n",
    "        dev_accs = [0.]\n",
    "        for epoch in range(NUM_EPOCH):\n",
    "            train_acc = self.train_epoch(train_iterator)\n",
    "            dev_acc = self.evaluate(dev_iterator)\n",
    "            print(\"epoch: {}\".format(epoch),\n",
    "                  \"train acc: {}\".format(train_acc[1]),\n",
    "                  \"train loss: {}\".format(train_acc[0]),\n",
    "                  \"dev acc: {}\".format(dev_acc[1]), \n",
    "                  \"dev loss:{}\".format(dev_acc[0]))\n",
    "            if dev_acc[1] > max(dev_accs):\n",
    "                best_model = copy.deepcopy(self)\n",
    "            dev_accs.append(dev_acc[1])\n",
    "        return best_model.model\n",
    "                \n",
    "    def evaluate(self, iterator):\n",
    "        epoch_loss = 0\n",
    "        epoch_acc = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            if (self.useRNN == True):\n",
    "                hidden = self.model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
    "            \n",
    "            for batch in iterator:\n",
    "                Q1 = batch.question1\n",
    "                Q2 = batch.question2\n",
    "                y = batch.is_duplicate\n",
    "                \n",
    "                y = y.float()\n",
    "                \n",
    "                predictions = self.model(Q1,Q2).squeeze(1)\n",
    "                loss = self.loss_fn(predictions, y)\n",
    "                acc = binary_accuracy(predictions, y)\n",
    "                epoch_loss += loss.item()\n",
    "                epoch_acc += acc.item()\n",
    "        \n",
    "        return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "yxma3cTuenCr",
    "outputId": "12901b12-7d90-48be-b8df-910556c40b55"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "nin = 600\n",
    "nhid1 = 200\n",
    "nhid2 = 200\n",
    "nhid3 = 200\n",
    "nout = 1\n",
    "NUM_EPOCH = 30\n",
    "useRNN = False\n",
    "model_1 = Model1(nin,nhid1,nhid2,nhid3,nout)\n",
    "model_1 = model_1.to(device)\n",
    "tm_1 = Training_module(model_1,useRNN)\n",
    "mymodel_1 = tm_1.train_model(train_iter,val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "73fv4PzGenCw",
    "outputId": "e8fb55a7-e5eb-4d89-c5ba-7763a354e1d4"
   },
   "outputs": [],
   "source": [
    "tst_loss_1, tst_acc_1 = tm_1.evaluate(test_iter)\n",
    "print(\"Tst Acc:\", tst_acc_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "obTH3EYgenC1"
   },
   "outputs": [],
   "source": [
    "#simple LSTM \n",
    "class Model2(nn.Module):\n",
    "    def __init__(self, input_dim, sentemb_dim,n_hidden1, n_hidden2, n_hidden3, n_out):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.qemb = nn.Embedding(VOCAB_SIZE,300)\n",
    "        self.qemb.weight = torch.nn.Parameter(TEXT.vocab.vectors, requires_grad = False)\n",
    "        \n",
    "        self.sentemb_dim = sentemb_dim\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        self.rnnq1 = nn.LSTM(self.input_dim, self.sentemb_dim)#, 1)\n",
    "        self.rnnq2 = nn.LSTM(self.input_dim, self.sentemb_dim)#, 1)\n",
    "        \n",
    "        self.l1 =nn.Linear(self.sentemb_dim * 2, self.n_hidden1)\n",
    "        self.l2 = nn.Linear(self.n_hidden1, self.n_hidden2)\n",
    "        self.l3 = nn.Linear(self.n_hidden2, self.n_hidden3)\n",
    "        self.out = nn.Linear(self.n_hidden3, n_out)\n",
    "        \n",
    "        self.l1.weight.data.uniform_(-1, 1)\n",
    "        self.l2.weight.data.uniform_(-1, 1)\n",
    "        self.l3.weight.data.uniform_(-1, 1)\n",
    "        self.out.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def forward(self, q1, q2):\n",
    "        q1out = self.qemb(q1)\n",
    "        q2out = self.qemb(q2)\n",
    "        \n",
    "        q1output, q1hidden_out = self.rnnq1(q1out)\n",
    "        q2output, q2hidden_out = self.rnnq2(q2out)\n",
    "        \n",
    "        q1a,q1b = q1hidden_out\n",
    "        q2a,q2b = q2hidden_out\n",
    "        \n",
    "        q1out = q1a.squeeze()\n",
    "        q2out = q2a.squeeze()\n",
    "        \n",
    "        output = torch.cat([q1out,q2out],dim=1)\n",
    "        \n",
    "        output = torch.tanh(self.l1(output))\n",
    "        output = torch.tanh(self.l2(output))\n",
    "        output = torch.tanh(self.l3(output))\n",
    "        return self.out(output)\n",
    "    \n",
    "    def init_hidden(self, bsz, requires_grad=True):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros((1, bsz, self.sentemb_dim), requires_grad=requires_grad),\n",
    "                    weight.new_zeros((1, bsz, self.sentemb_dim), requires_grad=requires_grad))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "1dvcImhgk79V",
    "outputId": "cf2892d3-c078-4ec1-bfd2-64c12c83e1aa"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "nin = 300\n",
    "sentemb_dim = 200\n",
    "nhid1 = 200\n",
    "nhid2 = 200\n",
    "nhid3 = 200\n",
    "nout = 1\n",
    "NUM_EPOCH = 20\n",
    "useRNN = True\n",
    "model_2 = Model2(nin,sentemb_dim,nhid1,nhid2,nhid3,nout)\n",
    "model_2 = model_2.to(device)\n",
    "tm_2 = Training_module(model_2,useRNN)\n",
    "mymodel_2 = tm_2.train_model(train_iter,val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DhNU_OoBiTqH",
    "outputId": "d275b6bd-4216-461d-de15-ef7de5b30b44"
   },
   "outputs": [],
   "source": [
    "tst_loss_2, tst_acc_2 = tm_2.evaluate(test_iter)\n",
    "print(\"Tst Acc:\", tst_acc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "KVOdbGFyh5OB"
   },
   "outputs": [],
   "source": [
    "#Siamese LSTM approach\n",
    "class Model3(nn.Module):\n",
    "    def __init__(self, input_dim, sentemb_dim,n_hidden1, n_hidden2, n_hidden3, n_out):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.qemb = nn.Embedding(VOCAB_SIZE,300)\n",
    "        self.qemb.weight = torch.nn.Parameter(TEXT.vocab.vectors, requires_grad = False)\n",
    "        \n",
    "        self.sentemb_dim = sentemb_dim\n",
    "        self.n_hidden1 = n_hidden1\n",
    "        self.n_hidden2 = n_hidden2\n",
    "        self.n_hidden3 = n_hidden3\n",
    "        self.n_out = n_out\n",
    "        \n",
    "        self.rnnq1 = nn.LSTM(self.input_dim, self.sentemb_dim)\n",
    "        self.rnnq2 = nn.LSTM(self.input_dim, self.sentemb_dim)\n",
    "        \n",
    "        self.l1 =nn.Linear(self.sentemb_dim * 4, self.n_hidden1)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.l2 = nn.Linear(self.n_hidden1, self.n_hidden2)\n",
    "        self.l3 = nn.Linear(self.n_hidden2, self.n_hidden3)\n",
    "        self.out = nn.Linear(self.n_hidden3, n_out)\n",
    "        \n",
    "        self.l1.weight.data.uniform_(-1, 1)\n",
    "        self.l2.weight.data.uniform_(-1, 1)\n",
    "        self.l3.weight.data.uniform_(-1, 1)\n",
    "        self.out.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def forward(self, q1, q2):\n",
    "        q1out = self.qemb(q1)\n",
    "        q2out = self.qemb(q2)\n",
    "        \n",
    "        q1output, q1hidden_out = self.rnnq1(q1out)\n",
    "        q2output, q2hidden_out = self.rnnq2(q2out)\n",
    "        \n",
    "        q1a,q1b = q1hidden_out\n",
    "        q2a,q2b = q2hidden_out\n",
    "        q1out = q1a.squeeze()\n",
    "        q2out = q2a.squeeze()\n",
    "        \n",
    "        #computing the distance information\n",
    "        qminus = torch.sub(q1out,q2out)\n",
    "        qelemwiseproduct = torch.mul(q1out,q2out)\n",
    "        \n",
    "        output = torch.cat([q1out,q2out,qminus,qelemwiseproduct],dim=1)\n",
    "        \n",
    "        output = torch.tanh(self.l1(output))\n",
    "        output = self.dropout(output)\n",
    "        output = torch.tanh(self.l2(output))\n",
    "        output = self.dropout(output)\n",
    "        return self.out(output)\n",
    "    \n",
    "    def init_hidden(self, bsz, requires_grad=True):\n",
    "        weight = next(self.parameters())\n",
    "        return (weight.new_zeros((1, bsz, self.sentemb_dim), requires_grad=requires_grad),\n",
    "                    weight.new_zeros((1, bsz, self.sentemb_dim), requires_grad=requires_grad))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "cF5aOp86h5OG",
    "outputId": "cd641a80-108a-4c22-d2e4-c5570d6c6ec3"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "nin = 300\n",
    "sentemb_dim = 200\n",
    "nhid1 = 200\n",
    "nhid2 = 200\n",
    "nhid3 = 200\n",
    "nout = 1\n",
    "NUM_EPOCH = 20\n",
    "useRNN = True\n",
    "model_3 = Model3(nin,sentemb_dim,nhid1,nhid2,nhid3,nout)\n",
    "model_3 = model_3.to(device)\n",
    "tm_3 = Training_module(model_3,useRNN)\n",
    "mymodel_3 = tm_3.train_model(train_iter,val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "t75wWCfih5OK",
    "outputId": "69c841b9-c592-4f82-d5c6-df53c3b5a352"
   },
   "outputs": [],
   "source": [
    "tst_loss_3, tst_acc_3 = tm_3.evaluate(test_iter)\n",
    "print(\"Tst Acc:\", tst_acc_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wS5aw7Ysh5OO"
   },
   "source": [
    "Attention with LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rtHRTEK3h5OP"
   },
   "outputs": [],
   "source": [
    "#Encoding of Question 1\n",
    "class Q1Module(nn.Module):\n",
    "    def __init__(self, input_dim, sentemb_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.qemb = nn.Embedding(VOCAB_SIZE,300)\n",
    "        self.qemb.weight = torch.nn.Parameter(TEXT.vocab.vectors, requires_grad = False)\n",
    "        \n",
    "        self.sentemb_dim = sentemb_dim\n",
    "        \n",
    "        self.rnnq1 = nn.LSTM(self.input_dim, self.sentemb_dim)\n",
    "        \n",
    "    def forward(self, q1):\n",
    "        q1out = self.qemb(q1)\n",
    "        q1output, q1hidden_out = self.rnnq1(q1out) \n",
    "        q1_h,q1_c = q1hidden_out\n",
    "        return q1output, q1hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "Lea7sqSUh5OT"
   },
   "outputs": [],
   "source": [
    "#Encoding of Question 2\n",
    "class Q2Module(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.qemb = nn.Embedding(VOCAB_SIZE,300)\n",
    "        self.qemb.weight = torch.nn.Parameter(TEXT.vocab.vectors, requires_grad = False)\n",
    "        \n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.rnnq2 = nn.LSTM(self.input_dim, self.out_dim)\n",
    "        \n",
    "    def forward(self, q2, hidden):\n",
    "        q2out = self.qemb(q2)\n",
    "        q2output, q2hidden_out = self.rnnq2(q2out, hidden)\n",
    "        q2_h,q2_c = q2hidden_out\n",
    "        return q2_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-7--GbXYh5OX"
   },
   "outputs": [],
   "source": [
    "#LSTM with attention\n",
    "#Notation is based on Rocktaschel et al\n",
    "#https://arxiv.org/pdf/1509.06664.pdf\n",
    "class Model4(nn.Module):\n",
    "    def __init__(self, Q1Module, Q2Module, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Q1Module = Q1Module\n",
    "        self.Q2Module = Q2Module\n",
    "        self.device = device\n",
    "        k = self.Q1Module.sentemb_dim * BATCH_SIZE #\n",
    "        \n",
    "        # Attention Parameters\n",
    "        self.W_y = nn.Parameter(torch.randn(k, k))\n",
    "        self.register_parameter('W_y', self.W_y)\n",
    "        \n",
    "        self.W_h = nn.Parameter(torch.randn(k, k)) \n",
    "        self.register_parameter('W_h', self.W_h)\n",
    "        \n",
    "        self.W_alpha = nn.Parameter(torch.randn(k, 1))\n",
    "        self.register_parameter('W_alpha', self.W_alpha)\n",
    "        \n",
    "        self.W_p = nn.Parameter(torch.randn(k, k))\n",
    "        self.register_parameter('W_p', self.W_p)\n",
    "        \n",
    "        self.W_x = nn.Parameter(torch.randn(k, k))  \n",
    "        self.register_parameter('W_x', self.W_x)\n",
    "        \n",
    "        self.l1 =nn.Linear(self.Q1Module.sentemb_dim, 1)\n",
    "        \n",
    "    def forward(self, q1, q2):\n",
    "        \n",
    "        q1output, q1hidden_out = self.Q1Module(q1)\n",
    "        q2_hidden_out = self.Q2Module(q2,q1hidden_out)\n",
    "        \n",
    "        #convert to 2d\n",
    "        Y = torch.t(q1output.view(q1output.shape[0],-1))\n",
    "\n",
    "        h_n = q2_hidden_out.view(-1,1)\n",
    "        \n",
    "        #L is a output dimensional vector of ones\n",
    "        L = torch.ones(q1output.shape[0]).to(self.device)\n",
    "\n",
    "        product_h_n = torch.mm(self.W_h,h_n)\n",
    "        product_h_n = product_h_n.view(-1)\n",
    "        \n",
    "        M = torch.tanh(torch.add(torch.mm(self.W_y,Y),torch.ger(product_h_n,L)))\n",
    "  \n",
    "        alpha = F.softmax(torch.mm(torch.t(self.W_alpha),M),dim=1)#rigth dim is 1\n",
    "     \n",
    "        r = torch.mm(Y,torch.t(alpha))\n",
    "      \n",
    "        h_star = torch.tanh(torch.add(torch.mm(self.W_p,r),torch.mm(self.W_x,h_n)))\n",
    "        \n",
    "        h_star = h_star.view(BATCH_SIZE,-1)\n",
    "        \n",
    "        outputs = self.l1(h_star)\n",
    " \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UndM8_BuenC7"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "INPUT_DIM = 300\n",
    "EMB_DIM = 200\n",
    "NUM_EPOCH = 30\n",
    "useRNN = False\n",
    "q1mod = Q1Module(INPUT_DIM, EMB_DIM)\n",
    "q2mod = Q2Module(INPUT_DIM, EMB_DIM)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_4 = Model4(q1mod, q2mod, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "WNZz_QtPenDA",
    "outputId": "6c1d7c2a-7adf-4da1-e9b4-bf2baab49eb3"
   },
   "outputs": [],
   "source": [
    "#initilaizing weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model_4.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 532
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "I1SUB1VbenDJ",
    "outputId": "a649f378-5a04-431e-d7d2-eb51a8ee0e3d"
   },
   "outputs": [],
   "source": [
    "tm_4 = Training_module(model_4,useRNN)\n",
    "mymodel_4 = tm_4.train_model(train_iter,val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "B_c06rrxenDQ",
    "outputId": "abf902f0-d92a-45a8-d157-a1c27c1dd9c1"
   },
   "outputs": [],
   "source": [
    "tst_loss_4, tst_acc_4 = tm_4.evaluate(test_iter)\n",
    "print(\"Tst Acc:\", tst_acc_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "ex8ePnfGenDV"
   },
   "outputs": [],
   "source": [
    "Word to word attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "jXPkzT7lenDZ"
   },
   "outputs": [],
   "source": [
    "#Encoding of Question 1\n",
    "class Q1ModuleModel5(nn.Module):\n",
    "    def __init__(self, input_dim, sentemb_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.qemb = nn.Embedding(VOCAB_SIZE,300)\n",
    "        self.qemb.weight = torch.nn.Parameter(TEXT.vocab.vectors, requires_grad = False)\n",
    "        \n",
    "        self.sentemb_dim = sentemb_dim\n",
    "        \n",
    "        self.rnnq1 = nn.LSTM(self.input_dim, self.sentemb_dim)\n",
    "        \n",
    "    def forward(self, q1):\n",
    "        q1out = self.qemb(q1)\n",
    "        q1output, q1hidden_out = self.rnnq1(q1out) \n",
    "        q1_h,q1_c = q1hidden_out\n",
    "        return q1output, q1hidden_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iKWV8aFrenDe"
   },
   "outputs": [],
   "source": [
    "#Encoding of Question 2\n",
    "class Q2ModuleModel5(nn.Module):\n",
    "    def __init__(self, input_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.qemb = nn.Embedding(VOCAB_SIZE,300)\n",
    "        self.qemb.weight = torch.nn.Parameter(TEXT.vocab.vectors, requires_grad = False)\n",
    "        \n",
    "        self.out_dim = out_dim\n",
    "        \n",
    "        self.rnnq2 = nn.LSTM(self.input_dim, self.out_dim)\n",
    "        \n",
    "    def forward(self, q2, hidden):\n",
    "        q2out = self.qemb(q2)\n",
    "        q2output, q2hidden_out = self.rnnq2(q2out, hidden)\n",
    "        return q2output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "zx5mfKCWenDj"
   },
   "outputs": [],
   "source": [
    "#This model is called model 5 version 1 in the paper\n",
    "#LSTM with word by word attention\n",
    "#Notation is based on Rocktaschel et al\n",
    "#https://arxiv.org/pdf/1509.06664.pdf\n",
    "class Model5(nn.Module):\n",
    "    def __init__(self, Q1Module, Q2Module, device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.Q1Module = Q1Module\n",
    "        self.Q2Module = Q2Module\n",
    "        self.device = device\n",
    "        k = self.Q1Module.sentemb_dim * BATCH_SIZE\n",
    "        \n",
    "        # Attention Parameters\n",
    "        self.W_y = nn.Parameter(torch.randn(k, k))\n",
    "        self.register_parameter('W_y', self.W_y)\n",
    "        \n",
    "        self.W_h = nn.Parameter(torch.randn(k, k)) \n",
    "        self.register_parameter('W_h', self.W_h)\n",
    "        \n",
    "        self.W_r = nn.Parameter(torch.randn(k, k)) \n",
    "        self.register_parameter('W_r', self.W_r)\n",
    "        \n",
    "        self.W_alpha = nn.Parameter(torch.randn(k, 1))\n",
    "        self.register_parameter('W_alpha', self.W_alpha)\n",
    "        \n",
    "        self.W_p = nn.Parameter(torch.randn(k, k))\n",
    "        self.register_parameter('W_p', self.W_p)\n",
    "        \n",
    "        self.W_t = nn.Parameter(torch.randn(k, k))\n",
    "        self.register_parameter('W_t', self.W_t)\n",
    "        \n",
    "        self.W_x = nn.Parameter(torch.randn(k, k))  \n",
    "        self.register_parameter('W_x', self.W_x)\n",
    "        \n",
    "        self.l1 =nn.Linear(self.Q1Module.sentemb_dim, 1)\n",
    "        \n",
    "    def forward(self, q1, q2):\n",
    "        \n",
    "        q1output, q1hidden_out = self.Q1Module(q1)\n",
    "        q2output = self.Q2Module(q2,q1hidden_out)\n",
    "        \n",
    "        #convert to 2d\n",
    "        Y = torch.t(q1output.view(q1output.shape[0],-1))\n",
    "\n",
    "        #L is a output dimensional vector of ones\n",
    "        L = torch.ones(q1output.shape[0]).to(self.device)\n",
    "        \n",
    "        r = torch.cuda.FloatTensor(self.Q1Module.sentemb_dim * BATCH_SIZE, 1).normal_()\n",
    "        \n",
    "        #loop through each word within a sentence\n",
    "        for i in range(q2output.shape[0]):\n",
    "            h_n = q2output[i,:,:]\n",
    "            h_n = h_n.view(-1,1)\n",
    "\n",
    "            product_h_n = torch.add(torch.mm(self.W_h,h_n),torch.mm(self.W_r,r))\n",
    "            product_h_n = product_h_n.view(-1)\n",
    "\n",
    "            M = torch.tanh(torch.add(torch.mm(self.W_y,Y),torch.ger(product_h_n,L)))\n",
    "\n",
    "            alpha = F.softmax(torch.mm(torch.t(self.W_alpha),M),dim=1)\n",
    "\n",
    "            r = torch.add(torch.mm(Y,torch.t(alpha)),torch.tanh(torch.mm(self.W_t,r)))\n",
    "\n",
    "        h_star = torch.tanh(torch.add(torch.mm(self.W_p,r),torch.mm(self.W_x,h_n)))\n",
    "        \n",
    "        h_star = h_star.view(BATCH_SIZE,-1)\n",
    "        \n",
    "        outputs = self.l1(h_star)\n",
    " \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "dBAjsNAsenDq"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "INPUT_DIM = 300\n",
    "EMB_DIM = 200\n",
    "NUM_EPOCH = 5\n",
    "useRNN = False\n",
    "q1mod5 = Q1ModuleModel5(INPUT_DIM, EMB_DIM)\n",
    "q2mod5 = Q2ModuleModel5(INPUT_DIM, EMB_DIM)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_5 = Model5(q1mod5, q2mod5, device).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "ZfEC7p2yenDu",
    "outputId": "c3d8daca-6647-43ca-9378-4ba0137a6e80"
   },
   "outputs": [],
   "source": [
    "#initilaizing weights\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        \n",
    "model_5.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "55NpSWuYenDy",
    "outputId": "913d4d64-35de-43f1-c797-f00650f76b72"
   },
   "outputs": [],
   "source": [
    "tm_5 = Training_module(model_5,useRNN)\n",
    "mymodel_5 = tm_5.train_model(train_iter,val_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "h-6uSntxenD2"
   },
   "outputs": [],
   "source": [
    "tst_loss_5, tst_acc_5 = tm_5.evaluate(test_iter)\n",
    "print(\"Tst Acc:\", tst_acc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "9SNzkk5RenD6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "iiKXpD_NenEB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "BKPlj5kJenEG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "UDiJfPfTenEL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rXeEuCLNh5Pm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Nlp_Project.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
